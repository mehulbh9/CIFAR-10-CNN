{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Eye-uBnQWgc8"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz9MhPEUMrkC"
      },
      "source": [
        "**Assignment**: Designing and Tuning a Convolutional Neural Network (CNN)\n",
        "\n",
        "**Assignment Description**: There are four parts to this assignment\n",
        "\n",
        "1.   Building a CNN\n",
        "2.   Training and Tuning a CNN\n",
        "3.   Trying Out a New Dataset\n",
        "4.   Open-Ended Exploration\n",
        "\n",
        "You will be largely guided through the first two parts. The third and fourth part are discussion based questions. \n",
        "\n",
        "**Before the experiment, make sure that you have GPU enabled. This setting can be found under *Tools --> Settings***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHvVs2GqXxNF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "03065866-9f0a-44f1-b9d8-4a31c9c87108"
      },
      "source": [
        "#Install Objax\n",
        "!pip --quiet install  objax\n",
        "import objax"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |█████▊                          | 10 kB 19.7 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 20 kB 19.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 30 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 40 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 51 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 57 kB 2.5 MB/s \n",
            "\u001b[?25h  Building wheel for objax (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqQf8f2RBDcx"
      },
      "source": [
        "import tensorflow as tf \n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import jax.numpy as jn\n",
        "import random \n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_7vcWRFO39r"
      },
      "source": [
        "##**Part 1. Building a CNN** \n",
        "\n",
        "Before we build our CNN model, let's first import a dataset. For our experiment, we load the CIFAR10 dataset from Tensorflow's dataset repository. The CIFAR10 dataset consists of 60,000 32x32 colour images in 10 classes, with 6000 images per class. The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks.\n",
        "\n",
        "After loading the dataset, we split the dataset into training, validation and test set. The dataset is originally stored as 50,000 training examples and 10,000 test examples. Instead, we will combine them together and make our own split.\n",
        "\n",
        "Do not change split ratio for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5VQDzs1XodT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "49abb3c9-8739-4f85-eaa1-4a7bfccb4074"
      },
      "source": [
        "#.load_data() by default returns a split between training and test set. \n",
        "# We then adjust the training set into a format that can be accepted by our CNN\n",
        "(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "X_train = X_train.transpose(0, 3, 1, 2) / 255.0\n",
        "Y_train = Y_train.flatten()\n",
        "X_test = X_test.transpose(0, 3, 1, 2) / 255.0\n",
        "Y_test = Y_test.flatten()\n",
        "\n",
        "np.random.seed(1)\n",
        "# To create a validation set, we first concate the original splitted dataset into a single dataset \n",
        "# then randomly shuffle the images and labels in the same way (seed = 1)\n",
        "X_data = np.concatenate([X_train, X_test], axis = 0)\n",
        "Y_data = np.concatenate([Y_train, Y_test], axis = 0)\n",
        "\n",
        "N = np.arange(len(X_data))\n",
        "np.random.shuffle(N)\n",
        "X_data = X_data[N]\n",
        "Y_data = Y_data[N]\n",
        "\n",
        "#Next, we partition the randomly shuffled dataset into training, validation and testset according a ratio\n",
        "train_ratio = 0.80\n",
        "valid_ratio = 0.1\n",
        "n_train = int(len(X_data) * train_ratio)\n",
        "n_valid = int(len(X_data) * valid_ratio)\n",
        "\n",
        "X_train, X_valid, X_test = X_data[:n_train], X_data[n_train:n_train+n_valid], X_data[n_train+n_valid:]\n",
        "Y_train, Y_valid, Y_test = Y_data[:n_train], Y_data[n_train:n_train+n_valid], Y_data[n_train+n_valid:]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 3s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szDmexFGT7Qs"
      },
      "source": [
        "\n",
        "Next we will construct a **Base Model**, which in our case is a small CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Eeh6jvfBV4p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "abe2d877-891b-4a08-ce25-f7ef7419bab0"
      },
      "source": [
        "class ConvNet(objax.Module):\n",
        "  def __init__(self, number_of_channels = 3, number_of_classes = 10):\n",
        "    self.conv_1 = objax.nn.Sequential([objax.nn.Conv2D(number_of_channels, 16, 2), objax.functional.relu])\n",
        "    self.conv_2 = objax.nn.Sequential([objax.nn.Conv2D(16, 32, 2), objax.functional.relu])\n",
        "    self.linear = objax.nn.Linear(32, number_of_classes)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = objax.functional.max_pool_2d(self.conv_1(x), 2, 2)\n",
        "    x = self.conv_2(x)\n",
        "  \n",
        "    x = x.mean((2,3)) #<--- global average pooling \n",
        "    x = self.linear(x)\n",
        "    return x\n",
        "\n",
        "#The following line creates the CNN\n",
        "model = ConvNet()\n",
        "#You can examine the architecture of our CNN by calling model.vars()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.vars()"
      ],
      "metadata": {
        "id": "74lHFyo-1PL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QgpeRSyjJrnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Crz2dQZBds89"
      },
      "source": [
        "Before we train our conv net, let's try to better understand concepts of convolution filter and linear layer. In the following, you will take the first very image of the training set, create a simple convolution routine, and show that our own routine matches what Objax returns. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Epa7ETf6XddH",
        "outputId": "3af89bac-f2db-48a5-8416-f701f08473ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "#Let's plot the first image in the training set.\n",
        "plt.imshow(X_train[0].transpose(1,2,0))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f58e565b890>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQjS06vgXZ7r"
      },
      "source": [
        "Next, we will pass our image through Objax's convolution routine. Carefully examine the following code and try to understand the dimension of the filter weights and the output. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFBbJHBpXXUc",
        "outputId": "7c30bcd6-ac1d-463e-df4b-b1ea0799cd73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# We append the first image with a batch size of 1 so it can be fed into a convolution layer\n",
        "my_image = np.expand_dims(X_train[0], 0)\n",
        "\n",
        "#Consider a very simple CNN filter with stride = 1 and no padding ('VALID').\n",
        "Conv2d = objax.nn.Conv2D(nin = 3, nout = 2, k = 1, strides = 1, padding = 'VALID', use_bias = False)\n",
        "\n",
        "filter_weights = Conv2d.w.value #This is the initial weight of the filter, which we gradually update when training, we ignore bias for now\n",
        "\n",
        "print(\"Filter weights:\", filter_weights)\n",
        "print(\"Conv output:\", Conv2d(my_image))\n",
        "print(\"Conv output shape:\", np.shape(Conv2d(my_image)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filter weights: [[[[ 0.7304995   0.68030566]\n",
            "   [-1.0113425  -0.19040492]\n",
            "   [-0.3848219   0.6740011 ]]]]\n",
            "Conv output: [[[[-0.10271908 -0.10010862 -0.09076816 ... -0.7420423  -0.73912483\n",
            "    -0.7374622 ]\n",
            "   [-0.12882358 -0.11180528 -0.11687267 ... -0.73105437 -0.7434611\n",
            "    -0.7408506 ]\n",
            "   [-0.13143402 -0.11577132 -0.12992492 ... -0.6991137  -0.7174033\n",
            "    -0.7156083 ]\n",
            "   ...\n",
            "   [-0.11485983 -0.10963893 -0.10963893 ... -0.11178758 -0.11439803\n",
            "    -0.12222939]\n",
            "   [-0.09658667 -0.09136578 -0.09136578 ... -0.1274503  -0.12483985\n",
            "    -0.12483985]\n",
            "   [-0.08614487 -0.08353443 -0.08614487 ... -0.16399662 -0.16138618\n",
            "    -0.15355481]]\n",
            "\n",
            "  [[ 0.1027988   0.09823449  0.08189838 ...  0.7525325   0.73205984\n",
            "     0.72146255]\n",
            "   [ 0.14844203  0.12636709  0.12754159 ...  0.7793448   0.76264036\n",
            "     0.758076  ]\n",
            "   [ 0.15300635  0.12562041  0.1503632  ...  0.7925383   0.7784523\n",
            "     0.7753319 ]\n",
            "   ...\n",
            "   [ 0.20083013  0.19170149  0.19170149 ...  0.27218533  0.27674964\n",
            "     0.29044262]\n",
            "   [ 0.16887988  0.15975124  0.15975124 ...  0.29957125  0.29500693\n",
            "     0.29500693]\n",
            "   [ 0.1506226   0.14605828  0.1506226  ...  0.36347175  0.35890743\n",
            "     0.34521446]]]]\n",
            "Conv output shape: (1, 2, 32, 32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsDg8yateSuH"
      },
      "source": [
        "**In the cells below, you will create your own convolution routine that takes in the image and the initial weights used by Objax's own convolution routine (Conv2d.w.value) and show that your convolution routine returns the same value than Objax's.**\n",
        "\n",
        "A simple implementation only requires 4 FOR loops. You may wish to draw inspiration from https://objax.readthedocs.io/en/latest/objax/nn.html?highlight=objax.nn.Conv2D#objax.nn.Conv2D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5B6K9gyXTAx"
      },
      "source": [
        "#Solution to the above problem\n",
        "\n",
        "def my_conv_net(image, weight):\n",
        "  N, Cin, H, W = image.shape\n",
        "  k, k2, Cin2, Cout = weight.shape\n",
        "  Hout = H-k+1\n",
        "  Wout = W-k+1\n",
        "  my_conv_output = np.empty(shape=(N, Cout, Hout, Wout))\n",
        "  for n in range(N):\n",
        "    for c in range(Cout):\n",
        "      for h in range(Hout):\n",
        "        for w in range(Wout):\n",
        "          image_Colors = image[n,:,h,w]\n",
        "          weight_slice = weight[0,0,:,c]\n",
        "          outpoint = jn.multiply(image_Colors, weight_slice)\n",
        "          my_conv_output[n,c,h,w] = jn.sum(outpoint)\n",
        "  return my_conv_output\n",
        "\n",
        "my_conv_output = my_conv_net(my_image, filter_weights)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "difference = (my_conv_output - Conv2d(my_image))"
      ],
      "metadata": {
        "id": "ATWWffCwr-oW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9dbovOwiVDE"
      },
      "source": [
        "The outputs of last convolution layer is typically rearranged so it can be fed into a linear layer. Check that calling .mean((2,3)) rearranges the output of your convolution routine by examining the shape of the output. (Not graded) Think about alternative ways of rearranging the output from the convolution layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7hngIUNXIMQ",
        "outputId": "012bda14-c7b6-400c-c4a0-f6bcb73506bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "#Check that .mean((2,3)) rearranges your image\n",
        "my_conv_output.mean((2,3))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.1400071 ,  0.35388899]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTWfvL0mis3D"
      },
      "source": [
        "Take your rearranged output and feed it into a linear layer of appropriate size. Here is an example:\n",
        "\n",
        "```\n",
        "Linear_Layer = objax.nn.Linear(N, 1)\n",
        "Y = Linear_Layer(X)\n",
        "```\n",
        "Next, extract the weights and bias of the linear layer using \n",
        "```\n",
        "Linear_Layer.w.value\n",
        "Linear_Layer.b.value\n",
        "```\n",
        "**Using these values, write one line of code that manually implements the linear layer. Show that it provides the same value as Objax's own linear layer.** \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq-TkFpgXDC7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "828b38d0-eb66-471a-e929-c112799fc1f0"
      },
      "source": [
        "\n",
        "\n",
        "#PUT YOUR CODE HERE\n",
        "\n",
        "#Linear Layer\n",
        "linear_layer = objax.nn.Linear(nin=32, nout=1, use_bias=True)\n",
        "np.shape(Conv2d(my_image))\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2, 32, 32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# w, b values\n",
        "\n",
        "w = linear_layer.w.value\n",
        "b = linear_layer.b.value\n",
        "\n",
        "w = np.asarray(w)\n",
        "b = np.asarray(b)\n",
        "\n",
        "#One line code here\n",
        "my_linear_output = np.dot(Conv2d(my_image), w) + b\n",
        "\n",
        "#now comparing\n",
        "difference = my_linear_output - linear_layer(Conv2d(my_image))\n",
        "\n",
        "#difference is almost zero"
      ],
      "metadata": {
        "id": "SQzsZLw92B8-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96QQXl3d2kZn"
      },
      "source": [
        "You have now completed Part 1 of the assignment. Good job!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kqaH75UUaSE"
      },
      "source": [
        "##**Part 2. Training and Tuning a CNN**\n",
        "\n",
        "The following starter code trains the neural network in Part 1. However, the optimizer and batch sampling routine are left for you to implement. Complete the lines that says #PUT YOUR CODE HERE#\n",
        "\n",
        "Afterwards, train the model, and observe the training/validation loss and accuracy plots. You should observe that the validation accuracy is low and stagnates after a few epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBcHWoCl0URZ"
      },
      "source": [
        "#Define loss function as averaged value of of cross entropies\n",
        "def loss_function(x, labels):\n",
        "    logit = model(x)\n",
        "    return objax.functional.loss.cross_entropy_logits_sparse(logit, labels).mean()\n",
        "\n",
        "#Define a prediction function\n",
        "predict = objax.Jit(lambda x: objax.functional.softmax(model(x)), model.vars()) \n",
        "\n",
        "#Create an object that can be used to calculate the gradient and value of loss_function\n",
        "gv= objax.GradValues(loss_function, model.vars())\n",
        "\n",
        "#Create an object that can be used to provide trainable variables in the model\n",
        "tv = objax.ModuleList(objax.TrainRef(x) for x in model.vars().subset(objax.TrainVar))\n",
        "\n",
        "#Training routine\n",
        "def train_op(x, y, learning_rate):\n",
        "    lr = learning_rate\n",
        "    gradient, loss_value = gv(x, y)   # calculate gradient and loss value \"backprop\"\n",
        "    #next we update the trainable parameter using SGD and similar procedure\n",
        "    for grad, params in zip(gradient, tv.vars()):\n",
        "      ####################\n",
        "      #PUT YOUR CODE HERE#\n",
        "      params.value = params.value - grad*lr\n",
        "      ####################                      \n",
        "    return loss_value                      # return loss value\n",
        "\n",
        "#make train_op (much) faster using JIT compilation\n",
        "train_op = objax.Jit(train_op, gv.vars() + tv.vars())"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGPpVTfG0Ug1"
      },
      "source": [
        "def train(EPOCHS = 20, BATCH = 32, LEARNING_RATE = 9e-4):\n",
        "  avg_train_loss_epoch = []\n",
        "  avg_val_loss_epoch = []\n",
        "  train_acc_epoch = []\n",
        "  val_acc_epoch = []\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "      avg_train_loss = 0 # (averaged) training loss per batch\n",
        "      avg_val_loss =  0  # (averaged) validation loss per batch\n",
        "      train_acc = 0      # training accuracy per batch\n",
        "      val_acc = 0        # validation accuracy per batch\n",
        "\n",
        "      # shuffle the examples prior to training to remove correlation \n",
        "      train_indices = np.arange(len(X_train)) \n",
        "      np.random.shuffle(train_indices)\n",
        "      for it in range(0, X_train.shape[0], BATCH):\n",
        "          batch = train_indices[it:it+BATCH]#PUT YOUR CODE HERE#\n",
        "          avg_train_loss += float(train_op(X_train[batch], Y_train[batch], LEARNING_RATE)[0]) * len(batch)\n",
        "          train_prediction = predict(X_train[batch]).argmax(1)\n",
        "          train_acc += (np.array(train_prediction).flatten() == Y_train[batch]).sum()\n",
        "      train_acc_epoch.append(train_acc/X_train.shape[0])\n",
        "      avg_train_loss_epoch.append(avg_train_loss/X_train.shape[0])\n",
        "\n",
        "      # run validation\n",
        "      val_indices = np.arange(len(X_valid)) \n",
        "      np.random.shuffle(val_indices)    \n",
        "      for it in range(0, X_valid.shape[0], BATCH):\n",
        "          batch = val_indices[it:it+BATCH]#PUT YOUR CODE HERE#\n",
        "          avg_val_loss += float(loss_function(X_valid[batch], Y_valid[batch])) * len(batch)\n",
        "          val_prediction = predict(X_valid[batch]).argmax(1)\n",
        "          val_acc += (np.array(val_prediction).flatten() == Y_valid[batch]).sum()\n",
        "      val_acc_epoch.append(val_acc/X_valid.shape[0])\n",
        "      avg_val_loss_epoch.append(avg_val_loss/X_valid.shape[0])\n",
        "\n",
        "      print('Epoch %04d  Training Loss %.2f Validation Loss %.2f Training Accuracy %.2f Validation Accuracy %.2f' % (epoch + 1, avg_train_loss/X_train.shape[0], avg_val_loss/X_valid.shape[0], 100*train_acc/X_train.shape[0], 100*val_acc/X_valid.shape[0]))\n",
        "  \n",
        "  #Plot training loss\n",
        "  plt.title(\"Train vs Validation Loss\")\n",
        "  plt.plot(avg_train_loss_epoch, label=\"Train\")\n",
        "  plt.plot(avg_val_loss_epoch, label=\"Validation\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()\n",
        "\n",
        "  plt.title(\"Train vs Validation Accuracy\")\n",
        "  plt.plot(train_acc_epoch, label=\"Train\")\n",
        "  plt.plot(val_acc_epoch, label=\"Validation\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Accuracy (%)\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YqWtV5VYW45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "6228b04f-e570-42cd-fa3b-c0ebe6083a5d"
      },
      "source": [
        "train()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-1e0dc9bfb7fe>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(EPOCHS, BATCH, LEARNING_RATE)\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m           \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBATCH\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#PUT YOUR CODE HERE#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m           \u001b[0mavg_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m           \u001b[0mtrain_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m           \u001b[0mtrain_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_prediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/objax/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;34m\"\"\"Call the compiled version of the function or module.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchanges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchanges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hluN6UIbNyj7"
      },
      "source": [
        "Follow the assignment handout for questions to be answered in this part of the assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8LXS4oHZA9M"
      },
      "source": [
        "#PUT YOUR CODE HERE (Use as many cells as you want)\n",
        "class ConvNetM1(objax.Module):\n",
        "  def __init__(self, number_of_channels = 3, number_of_classes = 10):\n",
        "    self.conv_1 = objax.nn.Sequential([objax.nn.Conv2D(number_of_channels, 16, 2), objax.functional.relu])\n",
        "    self.conv_2 = objax.nn.Sequential([objax.nn.Conv2D(16, 64, 2), objax.functional.relu])\n",
        "    self.linear = objax.nn.Linear(64, number_of_classes)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = objax.functional.max_pool_2d(self.conv_1(x), 2, 2)\n",
        "\n",
        "    x = self.conv_2(x)\n",
        "    x = x.mean((2,3)) #<--- global average pooling \n",
        "\n",
        "    x = self.linear(x)\n",
        "    return x\n",
        "\n",
        "#The following line creates the CNN\n",
        "modelM1 = ConvNetM1()\n",
        "#You can examine the architecture of our CNN by calling model.vars()\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define loss function as averaged value of cross entropies\n",
        "def loss_function(x, labels):\n",
        "    logit = modelM1(x) # instance of the ConvNet Class\n",
        "    return objax.functional.loss.cross_entropy_logits_sparse(logit, labels).mean()\n",
        "\n",
        "#Define a prediction function\n",
        "predict = objax.Jit(lambda x: objax.functional.softmax(modelM1(x)), modelM1.vars()) \n",
        "\n",
        "#Create an object that can be used to calculate the gradient and value of loss_function\n",
        "gv= objax.GradValues(loss_function, modelM1.vars())\n",
        "\n",
        "#Create an object that can be used to provide trainable variables in the model\n",
        "tv = objax.ModuleList(objax.TrainRef(x) for x in modelM1.vars().subset(objax.TrainVar))\n",
        "\n",
        "#Training routine\n",
        "def train_op(x, y, learning_rate):\n",
        "    lr = learning_rate\n",
        "    gradient, loss_value = gv(x, y)   # calculate gradient and loss value \"backprop\"\n",
        "    #next we update the trainable parameter using SGD and similar procedure\n",
        "    for grad, params in zip(gradient, tv.vars()):\n",
        "      params.value = params.value - lr * grad                      \n",
        "    return loss_value                      # return loss value\n",
        "\n",
        "#make train_op (much) faster using JIT compilation\n",
        "train_op = objax.Jit(train_op, gv.vars() + tv.vars())"
      ],
      "metadata": {
        "id": "f9rHFPdHt5BP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# H1 = {7, 0.01, 35, 64}\n",
        "\n",
        "def train_M1(EPOCHS = 35, BATCH = 7, LEARNING_RATE = 0.01):\n",
        "  avg_train_loss_epoch = []\n",
        "  avg_val_loss_epoch = []\n",
        "  train_acc_epoch = []\n",
        "  val_acc_epoch = []\n",
        "\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "      avg_train_loss = 0 # (averaged) training loss per batch\n",
        "      avg_val_loss = 0  # (averaged) validation loss per batch\n",
        "      train_acc = 0      # training accuracy per batch\n",
        "      val_acc = 0        # validation accuracy per batch\n",
        "\n",
        "\n",
        "\n",
        "      # shuffle the examples prior to training to remove correlation \n",
        "      train_indices = np.arange(len(X_train)) \n",
        "      np.random.shuffle(train_indices)\n",
        "\n",
        "\n",
        "      for it in range(0, X_train.shape[0], BATCH):\n",
        "          batch = train_indices[it:it+BATCH]\n",
        "          avg_train_loss += float(train_op(X_train[batch], Y_train[batch], LEARNING_RATE)[0]) * len(batch)\n",
        "          train_prediction = predict(X_train[batch]).argmax(1)\n",
        "          train_acc += (np.array(train_prediction).flatten() == Y_train[batch]).sum()\n",
        "      \n",
        "      train_acc_epoch.append(train_acc/X_train.shape[0])\n",
        "      avg_train_loss_epoch.append(avg_train_loss/X_train.shape[0])\n",
        "\n",
        "\n",
        "\n",
        "      # run validation\n",
        "\n",
        "      val_indices = np.arange(len(X_valid)) \n",
        "      np.random.shuffle(val_indices)    \n",
        "      for it in range(0, X_valid.shape[0], BATCH):\n",
        "\n",
        "          batch = val_indices[it:it+BATCH]\n",
        "\n",
        "          avg_val_loss += float(loss_function(X_valid[batch], Y_valid[batch])) * len(batch)\n",
        "          val_prediction = predict(X_valid[batch]).argmax(1)\n",
        "          val_acc += (np.array(val_prediction).flatten() == Y_valid[batch]).sum()\n",
        "      val_acc_epoch.append(val_acc/X_valid.shape[0])\n",
        "      avg_val_loss_epoch.append(avg_val_loss/X_valid.shape[0])\n",
        "\n",
        "      print('Epoch %04d  Training Loss %.2f Validation Loss %.2f Training Accuracy %.2f Validation Accuracy %.2f' % (epoch + 1, avg_train_loss/X_train.shape[0], avg_val_loss/X_valid.shape[0], 100*train_acc/X_train.shape[0], 100*val_acc/X_valid.shape[0]))\n",
        "\n",
        "\n",
        "  #Plot training loss\n",
        "  plt.title(\"Train vs Validation Loss\")\n",
        "  plt.plot(avg_train_loss_epoch, label=\"Train\")\n",
        "  plt.plot(avg_val_loss_epoch, label=\"Validation\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()\n",
        "\n",
        "  plt.title(\"Train vs Validation Accuracy\")\n",
        "  plt.plot(train_acc_epoch, label=\"Train\")\n",
        "  plt.plot(val_acc_epoch, label=\"Validation\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Accuracy (%)\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "E-z4RidEt-Tq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_M1()"
      ],
      "metadata": {
        "id": "HaVpc-Z6uDIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNetM2(objax.Module):\n",
        "  def __init__(self, number_of_channels = 3, number_of_classes = 10):\n",
        "    self.conv_1 = objax.nn.Sequential([objax.nn.Conv2D(number_of_channels, 16, 2), objax.functional.relu])\n",
        "    self.conv_2 = objax.nn.Sequential([objax.nn.Conv2D(16, 16, 2), objax.functional.relu])\n",
        "    self.linear = objax.nn.Linear(16, number_of_classes)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = objax.functional.max_pool_2d(self.conv_1(x), 2, 2)\n",
        "\n",
        "    x = self.conv_2(x)\n",
        "    x = x.mean((2,3)) #<--- global average pooling \n",
        "\n",
        "    x = self.linear(x)\n",
        "    return x\n",
        "\n",
        "#The following line creates the CNN\n",
        "modelM2 = ConvNetM2()\n",
        "#You can examine the architecture of our CNN by calling model.vars()"
      ],
      "metadata": {
        "id": "KEkldDsDuRRH"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define loss function as averaged value of cross entropies\n",
        "def loss_function(x, labels):\n",
        "    logit = modelM2(x) # instance of the ConvNet Class\n",
        "    return objax.functional.loss.cross_entropy_logits_sparse(logit, labels).mean()\n",
        "\n",
        "#Define a prediction function\n",
        "predict = objax.Jit(lambda x: objax.functional.softmax(modelM2(x)), modelM2.vars()) \n",
        "\n",
        "#Create an object that can be used to calculate the gradient and value of loss_function\n",
        "gv= objax.GradValues(loss_function, modelM2.vars())\n",
        "\n",
        "#Create an object that can be used to provide trainable variables in the model\n",
        "tv = objax.ModuleList(objax.TrainRef(x) for x in modelM2.vars().subset(objax.TrainVar))\n",
        "\n",
        "#Training routine\n",
        "def train_op(x, y, learning_rate):\n",
        "    lr = learning_rate\n",
        "    gradient, loss_value = gv(x, y)   # calculate gradient and loss value \"backprop\"\n",
        "    #next we update the trainable parameter using SGD and similar procedure\n",
        "    for grad, params in zip(gradient, tv.vars()):\n",
        "      params.value = params.value - lr * grad                      \n",
        "    return loss_value                      # return loss value\n",
        "\n",
        "#make train_op (much) faster using JIT compilation\n",
        "train_op = objax.Jit(train_op, gv.vars() + tv.vars())"
      ],
      "metadata": {
        "id": "SeN9D2mguWXT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## H2 = {25, 0.001, 75, 16}\n",
        "\n",
        "\n",
        "def train_M2(EPOCHS = 75, BATCH = 25, LEARNING_RATE = 0.001):\n",
        "  avg_train_loss_epoch = []\n",
        "  avg_val_loss_epoch = []\n",
        "  train_acc_epoch = []\n",
        "  val_acc_epoch = []\n",
        "\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "      avg_train_loss = 0 # (averaged) training loss per batch\n",
        "      avg_val_loss = 0  # (averaged) validation loss per batch\n",
        "      train_acc = 0      # training accuracy per batch\n",
        "      val_acc = 0        # validation accuracy per batch\n",
        "\n",
        "\n",
        "\n",
        "      # shuffle the examples prior to training to remove correlation \n",
        "      train_indices = np.arange(len(X_train)) \n",
        "      np.random.shuffle(train_indices)\n",
        "\n",
        "\n",
        "      for it in range(0, X_train.shape[0], BATCH):\n",
        "          batch = train_indices[it:it+BATCH]\n",
        "          avg_train_loss += float(train_op(X_train[batch], Y_train[batch], LEARNING_RATE)[0]) * len(batch)\n",
        "          train_prediction = predict(X_train[batch]).argmax(1)\n",
        "          train_acc += (np.array(train_prediction).flatten() == Y_train[batch]).sum()\n",
        "      \n",
        "      train_acc_epoch.append(train_acc/X_train.shape[0])\n",
        "      avg_train_loss_epoch.append(avg_train_loss/X_train.shape[0])\n",
        "\n",
        "\n",
        "\n",
        "      # run validation\n",
        "\n",
        "      val_indices = np.arange(len(X_valid)) \n",
        "      np.random.shuffle(val_indices)    \n",
        "      for it in range(0, X_valid.shape[0], BATCH):\n",
        "\n",
        "          batch = val_indices[it:it+BATCH]\n",
        "\n",
        "          avg_val_loss += float(loss_function(X_valid[batch], Y_valid[batch])) * len(batch)\n",
        "          val_prediction = predict(X_valid[batch]).argmax(1)\n",
        "          val_acc += (np.array(val_prediction).flatten() == Y_valid[batch]).sum()\n",
        "      val_acc_epoch.append(val_acc/X_valid.shape[0])\n",
        "      avg_val_loss_epoch.append(avg_val_loss/X_valid.shape[0])\n",
        "\n",
        "      print('Epoch %04d  Training Loss %.2f Validation Loss %.2f Training Accuracy %.2f Validation Accuracy %.2f' % (epoch + 1, avg_train_loss/X_train.shape[0], avg_val_loss/X_valid.shape[0], 100*train_acc/X_train.shape[0], 100*val_acc/X_valid.shape[0]))\n",
        "\n",
        "\n",
        "  #Plot training loss\n",
        "  plt.title(\"Train vs Validation Loss\")\n",
        "  plt.plot(avg_train_loss_epoch, label=\"Train\")\n",
        "  plt.plot(avg_val_loss_epoch, label=\"Validation\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()\n",
        "\n",
        "  plt.title(\"Train vs Validation Accuracy\")\n",
        "  plt.plot(train_acc_epoch, label=\"Train\")\n",
        "  plt.plot(val_acc_epoch, label=\"Validation\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Accuracy (%)\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "PhoCA4m2ubzA"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_M2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7oGJVg20udFZ",
        "outputId": "c4c45ac8-ecb5-4d51-f8f7-954e0c91be2e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.train_M2(EPOCHS=75, BATCH=25, LEARNING_RATE=0.001)>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "npF2d5lu6qGv",
        "outputId": "0dfab356-918e-4789-a113-d0718f20a815"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/objax/nn/layers.py\u001b[0m in \u001b[0;36mrun_layer\u001b[0;34m(layer, f, args, kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/objax/nn/layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mnin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         assert x.shape[1] == nin, (f'Attempting to convolve an input with {x.shape[1]} input channels '\n\u001b[0m\u001b[1;32m    197\u001b[0m                                    \u001b[0;34mf'when the convolution expects {nin} channels. For reference, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Attempting to convolve an input with 32 input channels when the convolution expects 3 channels. For reference, self.w.value.shape=(2, 2, 3, 16) and x.shape=(3, 32, 32).",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-9d932435d16e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mtest_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mtest_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_prediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mY_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-aa94483a53ce>\u001b[0m in \u001b[0;36mloss_function\u001b[0;34m(x, labels)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Define loss function as averaged value of cross entropies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelM2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# instance of the ConvNet Class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mobjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_logits_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-62fdbd962b2d>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/objax/nn/layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/objax/nn/layers.py\u001b[0m in \u001b[0;36mrun_layer\u001b[0;34m(layer, f, args, kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Sequential layer[{layer}] {f} {e}'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mJaxArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mJaxArray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Sequential layer[0] objax.nn.Conv2D(nin=3, nout=16, k=(2, 2), strides=(1, 1), dilations=(1, 1), groups=1, padding='SAME', use_bias=True, w_init=kaiming_normal(*, gain=1)) Attempting to convolve an input with 32 input channels when the convolution expects 3 channels. For reference, self.w.value.shape=(2, 2, 3, 16) and x.shape=(3, 32, 32)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I would pick model 2 over the other as it has lowest validation error"
      ],
      "metadata": {
        "id": "3SuMfcBxxpU0"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XOXmJiGZF78"
      },
      "source": [
        "You have now completed Part 2 of the assignment. Good job!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cno9fjO1o2Jq"
      },
      "source": [
        "##**Part 3. Trying Out a New Dataset**\n",
        "\n",
        "See the handout for instructions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "wPbue4RqF3sQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#.load_data() by default returns a split between training and test set. \n",
        "# We then adjust the training set into a format that can be accepted by our CNN\n",
        "\n",
        "(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.mnist.load_data()\n",
        "# X_train = X_train.transpose(0, 3, 1, 2) / 255.0\n",
        "# Y_train = Y_train.flatten()\n",
        "# X_test = X_test.transpose(0, 3, 1, 2) / 255.0\n",
        "# Y_test = Y_test.flatten()\n",
        "\n",
        "np.random.seed(1)\n",
        "# To create a validation set, we first concate the original splitted dataset into a single dataset \n",
        "# then randomly shuffle the images and labels in the same way (seed = 1)\n",
        "X_data = np.concatenate([X_train, X_test], axis = 0)\n",
        "Y_data = np.concatenate([Y_train, Y_test], axis = 0)\n",
        "\n",
        "N = np.arange(len(X_data))\n",
        "np.random.shuffle(N)\n",
        "X_data = X_data[N]\n",
        "Y_data = Y_data[N]\n",
        "\n",
        "#Next, we partition the randomly shuffled dataset into training, validation and testset according a ratio\n",
        "train_ratio = 0.80\n",
        "valid_ratio = 0.1\n",
        "n_train = int(len(X_data) * train_ratio)\n",
        "n_valid = int(len(X_data) * valid_ratio)\n",
        "\n",
        "X_train, X_valid, X_test = X_data[:n_train], X_data[n_train:n_train+n_valid], X_data[n_train+n_valid:]\n",
        "Y_train, Y_valid, Y_test = Y_data[:n_train], Y_data[n_train:n_train+n_valid], Y_data[n_train+n_valid:]"
      ],
      "metadata": {
        "id": "krI8J8H1IeOx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "813ba46d-82ff-414f-ed34-9cd7f9e23b28"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(objax.Module):\n",
        "  def __init__(self, number_of_channels = 3, number_of_classes = 10):\n",
        "    self.conv_1 = objax.nn.Sequential([objax.nn.Conv2D(number_of_channels, 16, 2), objax.functional.relu])\n",
        "    self.conv_2 = objax.nn.Sequential([objax.nn.Conv2D(16, 32, 2), objax.functional.relu])\n",
        "    self.linear = objax.nn.Linear(32, number_of_classes)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = objax.functional.max_pool_2d(self.conv_1(x), 2, 2)\n",
        "    x = self.conv_2(x)\n",
        "  \n",
        "    x = x.mean((2,3)) #<--- global average pooling \n",
        "    x = self.linear(x)\n",
        "    return x\n",
        "\n",
        "#The following line creates the CNN\n",
        "model = ConvNet()\n",
        "#You can examine the architecture of our CNN by calling model.vars()"
      ],
      "metadata": {
        "id": "3aQPZX67JV7v"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.vars()"
      ],
      "metadata": {
        "id": "_dbJWlcpJuDK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b2cf2c7e-27f4-48f5-a437-1423b19accd8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'(ConvNet).conv_1(Sequential)[0](Conv2D).b': objax.TrainVar(DeviceArray([[[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]]], dtype=float32), reduce=reduce_mean),\n",
              " '(ConvNet).conv_1(Sequential)[0](Conv2D).w': objax.TrainVar(DeviceArray([[[[-0.49783123,  0.05491344,  0.05481252,  0.15915959,\n",
              "                  0.09677444,  0.16445793, -0.19755745, -0.13090663,\n",
              "                  0.11064189,  0.35113814,  0.11056957, -0.3743479 ,\n",
              "                  0.07479615,  0.3063457 ,  0.03331185,  0.02988443],\n",
              "                [-0.1206367 ,  0.17287579, -0.27509516,  0.22232825,\n",
              "                  0.0648784 ,  0.11360257,  0.35161892, -0.2763335 ,\n",
              "                  0.21090616,  0.37282857, -0.31829008,  0.10255507,\n",
              "                  0.83104056,  0.4735009 ,  0.2733064 ,  0.23843218],\n",
              "                [ 0.12542862, -0.24819188,  0.00559252,  0.02936969,\n",
              "                  0.19867884,  0.19637452,  0.11754108,  0.35550687,\n",
              "                 -0.06865977,  0.04171978,  0.07382798,  0.20558408,\n",
              "                  0.19804128,  0.21013868,  0.00328237, -0.14921376]],\n",
              "               [[-0.46173397, -0.24037991, -0.54995024,  0.26039442,\n",
              "                  0.4009132 , -0.02262037, -0.07737535,  0.5268714 ,\n",
              "                  0.05710221, -0.20916   , -0.2239796 , -0.0683652 ,\n",
              "                 -0.03504497, -0.10082386, -0.33768007, -0.3391354 ],\n",
              "                [ 0.24185915,  0.12939349,  0.5546725 , -0.08899114,\n",
              "                 -0.34155256,  0.41563278, -0.21823221,  0.1121263 ,\n",
              "                 -0.3222117 , -0.2225357 , -0.3662564 ,  0.11138466,\n",
              "                  0.06861944, -0.3002726 ,  0.2010615 , -0.529092  ],\n",
              "                [-0.4861035 ,  0.14012192,  0.49118015,  0.01562787,\n",
              "                  0.01030113,  0.5988927 , -0.18713021, -0.52454615,\n",
              "                 -0.10922985, -0.3019374 ,  0.2839072 ,  0.23283133,\n",
              "                  0.36505026,  0.11595841, -0.13272211, -0.61168027]]],\n",
              "              [[[-0.23228869, -0.23386684, -0.07516003,  0.3319735 ,\n",
              "                 -0.02331394, -0.36107412, -0.3160257 ,  0.33640862,\n",
              "                  0.3774382 , -0.30612832,  0.43370232,  0.02827036,\n",
              "                  0.12029956, -0.3885868 , -0.35240477, -0.42470875],\n",
              "                [-0.02901399, -0.50510466,  0.2900967 ,  0.51747   ,\n",
              "                 -0.02732849, -0.15658256, -0.14303066, -0.19093163,\n",
              "                  0.4370792 ,  0.09714031, -0.15406674, -0.00722811,\n",
              "                  0.21344599, -0.3115972 ,  0.6343355 , -0.19941477],\n",
              "                [-0.23660874,  0.13706487,  0.21163297, -0.1137416 ,\n",
              "                 -0.0782015 ,  0.06312327,  0.08146454,  0.10278104,\n",
              "                  0.19991928,  0.02582712,  0.06016523,  0.5678249 ,\n",
              "                  0.07323054,  0.33784056,  0.4519415 ,  0.7245857 ]],\n",
              "               [[-0.51603025, -0.23262009, -0.17509514,  0.253867  ,\n",
              "                 -0.48864466,  0.10137611,  0.24241441, -0.09440213,\n",
              "                 -0.42147878, -0.05192436, -0.08896291,  0.19885805,\n",
              "                 -0.36662182,  0.17164026,  0.04291509, -0.02251611],\n",
              "                [-0.0314226 ,  0.08864389,  0.49756336, -0.6158516 ,\n",
              "                 -0.5120317 ,  0.07839369, -0.31631452, -0.03074508,\n",
              "                 -0.2206037 ,  0.24459198,  0.1001512 ,  0.10538449,\n",
              "                  0.22515173, -0.23672113, -0.04068169,  0.36277047],\n",
              "                [-0.686461  ,  0.05087608, -0.0255897 , -0.12272845,\n",
              "                 -0.53593326,  0.28550637, -0.12948711,  0.25939226,\n",
              "                 -0.40354502,  0.14700171,  0.2627372 ,  0.13043658,\n",
              "                  0.06342087,  0.08786511, -0.55525523,  0.2653245 ]]]],            dtype=float32), reduce=reduce_mean),\n",
              " '(ConvNet).conv_2(Sequential)[0](Conv2D).b': objax.TrainVar(DeviceArray([[[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]],\n",
              "              [[0.]]], dtype=float32), reduce=reduce_mean),\n",
              " '(ConvNet).conv_2(Sequential)[0](Conv2D).w': objax.TrainVar(DeviceArray([[[[ 0.0792159 ,  0.15747371,  0.06401204, ...,\n",
              "                 -0.17682675,  0.19234365,  0.27712268],\n",
              "                [ 0.00978582, -0.12251885,  0.07334221, ...,\n",
              "                  0.03569439, -0.13152234,  0.01244445],\n",
              "                [ 0.12816353,  0.04415412, -0.10547303, ...,\n",
              "                  0.14620002, -0.12298938, -0.07753317],\n",
              "                ...,\n",
              "                [-0.13288687,  0.23011848,  0.03482577, ...,\n",
              "                  0.05239462,  0.21378815, -0.02400324],\n",
              "                [ 0.00962319, -0.18917021, -0.11962917, ...,\n",
              "                 -0.1292916 , -0.12082145,  0.19254707],\n",
              "                [-0.07217373, -0.20590276, -0.06908357, ...,\n",
              "                 -0.18634784,  0.1362071 ,  0.09454344]],\n",
              "               [[ 0.0141959 , -0.02242458,  0.23095223, ...,\n",
              "                 -0.08664506, -0.21022516, -0.1076716 ],\n",
              "                [-0.16586657, -0.07472806, -0.05923181, ...,\n",
              "                  0.03805161, -0.0161498 ,  0.21298854],\n",
              "                [ 0.05739973,  0.03082387, -0.04003966, ...,\n",
              "                 -0.12741427, -0.05825269, -0.14913206],\n",
              "                ...,\n",
              "                [ 0.03507765, -0.02226958, -0.258416  , ...,\n",
              "                 -0.17421472,  0.23729847, -0.09021644],\n",
              "                [ 0.1045608 , -0.11432884, -0.07639456, ...,\n",
              "                  0.09487305, -0.00163507, -0.12851094],\n",
              "                [ 0.33978242, -0.18892723,  0.11123721, ...,\n",
              "                 -0.16070727, -0.11668343, -0.09280176]]],\n",
              "              [[[-0.04734867, -0.04071187, -0.11965521, ...,\n",
              "                  0.0651528 ,  0.16649243,  0.12015357],\n",
              "                [ 0.07964858,  0.02110543, -0.01478219, ...,\n",
              "                  0.1597444 , -0.07542901, -0.09591672],\n",
              "                [-0.01802132, -0.00139757, -0.03392859, ...,\n",
              "                 -0.00901291,  0.06909747, -0.03232289],\n",
              "                ...,\n",
              "                [-0.21851104, -0.14007024, -0.09623079, ...,\n",
              "                 -0.13694486, -0.0077487 , -0.15761526],\n",
              "                [-0.00747594,  0.3081178 , -0.00603597, ...,\n",
              "                 -0.07546289, -0.13440311, -0.1257082 ],\n",
              "                [-0.02110329,  0.11549713,  0.10667805, ...,\n",
              "                  0.03677799,  0.09917167, -0.20384507]],\n",
              "               [[ 0.33520722,  0.05844123, -0.07519053, ...,\n",
              "                  0.17413038,  0.15396874, -0.02405503],\n",
              "                [-0.0960245 , -0.00174654,  0.02456112, ...,\n",
              "                  0.14458996, -0.03962625, -0.17335027],\n",
              "                [ 0.0452411 ,  0.05974237,  0.126111  , ...,\n",
              "                 -0.04783979,  0.05118451, -0.05397284],\n",
              "                ...,\n",
              "                [-0.01332668,  0.06353441, -0.13036509, ...,\n",
              "                  0.07943217,  0.17061903, -0.09664304],\n",
              "                [-0.12043149, -0.04856365,  0.06728873, ...,\n",
              "                 -0.07987309,  0.03504063,  0.22008428],\n",
              "                [-0.09919453, -0.06623584, -0.07459234, ...,\n",
              "                 -0.03431306, -0.19298811,  0.17896125]]]], dtype=float32), reduce=reduce_mean),\n",
              " '(ConvNet).linear(Linear).b': objax.TrainVar(DeviceArray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), reduce=reduce_mean),\n",
              " '(ConvNet).linear(Linear).w': objax.TrainVar(DeviceArray([[-0.15270887, -0.2061034 ,  0.14179462, -0.16911758,\n",
              "                0.2859449 , -0.09476715,  0.23291534, -0.32130837,\n",
              "               -0.07899796,  0.0688675 ],\n",
              "              [ 0.29772463, -0.093891  , -0.03133785,  0.3455356 ,\n",
              "               -0.11740624, -0.06169633,  0.0624177 , -0.44893038,\n",
              "                0.27053556,  0.10559157],\n",
              "              [-0.16473803, -0.02313506, -0.12364285,  0.01687071,\n",
              "               -0.05797248, -0.06870993, -0.08342696,  0.09990104,\n",
              "                0.18494534, -0.11383519],\n",
              "              [ 0.15444008,  0.3904371 , -0.32005972,  0.04485963,\n",
              "                0.27814212, -0.17371133, -0.05828529, -0.21545772,\n",
              "                0.22060674,  0.162683  ],\n",
              "              [ 0.24184652, -0.07811107,  0.09678986,  0.31028712,\n",
              "                0.42870316,  0.12922572, -0.02606248,  0.1256328 ,\n",
              "                0.01693382, -0.4107425 ],\n",
              "              [ 0.06310716, -0.15868242,  0.03077996, -0.03715666,\n",
              "                0.14599931, -0.371858  , -0.03344738, -0.17193583,\n",
              "                0.04408609,  0.20795943],\n",
              "              [ 0.43239757,  0.2082097 ,  0.13438797,  0.43422142,\n",
              "               -0.19848345, -0.19175452,  0.04636277, -0.2780238 ,\n",
              "               -0.11220919,  0.29975054],\n",
              "              [-0.1882738 ,  0.1085307 , -0.11665123, -0.17327431,\n",
              "               -0.1607874 ,  0.0226973 ,  0.3394089 , -0.01330322,\n",
              "                0.16140905, -0.14228183],\n",
              "              [-0.28615266,  0.45055905,  0.10595603, -0.11467129,\n",
              "                0.18044412, -0.04034277,  0.4350989 , -0.07704222,\n",
              "                0.03979775, -0.14713971],\n",
              "              [ 0.3449898 , -0.17306429,  0.09401023, -0.09549713,\n",
              "               -0.2973519 , -0.10927349,  0.02884064,  0.13838285,\n",
              "               -0.10221721,  0.43921596],\n",
              "              [ 0.38269463, -0.3839193 , -0.26792297, -0.09407093,\n",
              "                0.07156878,  0.33551145,  0.5885502 , -0.26509824,\n",
              "                0.06706119, -0.3427113 ],\n",
              "              [ 0.23193859, -0.00577038, -0.09831999, -0.23203143,\n",
              "               -0.07266089, -0.02255488, -0.4756515 ,  0.3292247 ,\n",
              "               -0.06913187,  0.2170261 ],\n",
              "              [ 0.09480874, -0.16994186, -0.18114743, -0.2767403 ,\n",
              "               -0.15246083,  0.00477579, -0.10394356,  0.06709755,\n",
              "               -0.01616796, -0.0421827 ],\n",
              "              [ 0.0661687 ,  0.06770515,  0.04242505, -0.00765294,\n",
              "               -0.30923533, -0.3451213 , -0.05814536,  0.2938726 ,\n",
              "                0.12124608, -0.2850684 ],\n",
              "              [-0.06130572,  0.4367118 ,  0.01786198,  0.08579798,\n",
              "               -0.08998898,  0.02992325,  0.07101639,  0.19304685,\n",
              "               -0.37798062, -0.14745662],\n",
              "              [ 0.15378882,  0.17436221,  0.47898754,  0.08854719,\n",
              "                0.10205086, -0.01407777, -0.04513795,  0.13041463,\n",
              "               -0.08185047,  0.19850855],\n",
              "              [-0.1363147 ,  0.21303666, -0.07299318, -0.2225078 ,\n",
              "                0.03844129,  0.41688955,  0.03023047, -0.28678042,\n",
              "                0.00819324,  0.31233248],\n",
              "              [-0.01517812,  0.26156455, -0.12168755, -0.3541663 ,\n",
              "               -0.5879027 , -0.3169108 ,  0.18343556, -0.28749904,\n",
              "               -0.47222862,  0.31498998],\n",
              "              [-0.1681446 ,  0.13540174,  0.32910168, -0.04303134,\n",
              "                0.02097461, -0.00314082, -0.29167205, -0.32636777,\n",
              "               -0.23509458, -0.19403629],\n",
              "              [-0.42350724,  0.15151936,  0.2849222 , -0.05841112,\n",
              "                0.33003953,  0.15822735,  0.18200622, -0.21116313,\n",
              "               -0.5484282 , -0.42245537],\n",
              "              [ 0.18565705,  0.15629491,  0.04180596, -0.22520351,\n",
              "                0.2084902 , -0.09077121,  0.03987917,  0.00436275,\n",
              "               -0.05920056,  0.08168675],\n",
              "              [-0.25394884,  0.1462653 , -0.00368034, -0.355995  ,\n",
              "                0.22731787,  0.11677374,  0.04474062, -0.00792137,\n",
              "                0.08612551, -0.06158104],\n",
              "              [-0.16841048,  0.28045195, -0.07851003,  0.07009039,\n",
              "                0.22894286, -0.00957817, -0.1524957 , -0.1652945 ,\n",
              "                0.12627766,  0.01415745],\n",
              "              [ 0.3575119 , -0.37774122, -0.18141113, -0.09862712,\n",
              "                0.00200768,  0.23508264, -0.03574177,  0.09793545,\n",
              "                0.17451791, -0.07638355],\n",
              "              [-0.50933856,  0.31476647, -0.13401207, -0.27381882,\n",
              "               -0.0009921 ,  0.34999204, -0.16310906,  0.5154048 ,\n",
              "               -0.02870616,  0.45359856],\n",
              "              [ 0.06090735,  0.1815954 , -0.1385589 , -0.25950447,\n",
              "                0.00309808,  0.4039147 ,  0.00425227,  0.05050712,\n",
              "               -0.23741773,  0.6497622 ],\n",
              "              [-0.49043295, -0.07186985, -0.03690032, -0.06165419,\n",
              "                0.09610332,  0.30487555,  0.20401885, -0.23900984,\n",
              "               -0.09303094, -0.13028124],\n",
              "              [-0.21588406,  0.5338186 , -0.07929666, -0.00570796,\n",
              "               -0.05210826, -0.13991998, -0.21382342,  0.13684672,\n",
              "                0.13626513, -0.10073649],\n",
              "              [ 0.0405639 , -0.01565178,  0.24349664,  0.2294995 ,\n",
              "                0.12850276,  0.14245218, -0.03776706, -0.00585583,\n",
              "                0.42670503,  0.04830071],\n",
              "              [-0.11108326, -0.13937688, -0.0108014 ,  0.05046198,\n",
              "               -0.20928277,  0.25701392,  0.24625944,  0.01360685,\n",
              "               -0.12514177,  0.08859356],\n",
              "              [-0.16685611,  0.05387811,  0.12099955,  0.26094046,\n",
              "                0.34724057, -0.11619661, -0.11923146,  0.913182  ,\n",
              "               -0.02079614,  0.11284372],\n",
              "              [-0.12318651,  0.2911644 ,  0.08574121,  0.088958  ,\n",
              "                0.29412517,  0.20727943,  0.2592932 ,  0.13729453,\n",
              "                0.14418833, -0.26755202]], dtype=float32), reduce=reduce_mean)}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's plot the first image in the training set.\n",
        "plt.imshow(X_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "0not8s9vJzCB",
        "outputId": "f5638390-7413-4469-9674-79abbf64dd6f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f58e4ada7d0>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNetMNIST(objax.Module):\n",
        "  def __init__(self, number_of_channels = 3, number_of_classes = 10):\n",
        "    self.conv_1 = objax.nn.Sequential([objax.nn.Conv2D(number_of_channels, 16, 2), objax.functional.relu])\n",
        "    self.conv_2 = objax.nn.Sequential([objax.nn.Conv2D(16, 16, 2), objax.functional.relu])\n",
        "    self.linear = objax.nn.Linear(16, number_of_classes)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = objax.functional.max_pool_2d(self.conv_1(x), 2, 2)\n",
        "\n",
        "    x = self.conv_2(x)\n",
        "    x = x.mean((2,3)) #<--- global average pooling \n",
        "\n",
        "    x = self.linear(x)\n",
        "    return x\n",
        "\n",
        "#The following line creates the CNN\n",
        "modelMNIST = ConvNetMNIST()\n",
        "#You can examine the architecture of our CNN by calling model.vars()"
      ],
      "metadata": {
        "id": "TJGPnPjAKFQA"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function_m3(x, labels):\n",
        "  logit = modelMNIST(x)\n",
        "  return objax.functional.loss.cross_entropy_logits_sparse(logit,labels).mean()\n",
        "\n",
        "predict_3 = objax.Jit(lambda x : objax.functional.softmax(modelMNIST(x)) , modelMNIST.vars())\n",
        "\n",
        "gv_3= objax.GradValues(loss_function_m3, modelMNIST.vars())\n",
        "\n",
        "tv_3 = objax.ModuleList(objax.TrainRef(x) for x in modelMNIST.vars().subset(objax.TrainVar))\n",
        "\n",
        "opt_3 = objax.optimizer.SGD(modelMNIST.vars())\n",
        "\n",
        "def train_op_3(x, y, learning_rate):\n",
        "  lr = learning_rate\n",
        "  g, loss_value = gv_3(x, y)  # returns gradients, loss\n",
        "  opt_3(lr, g)\n",
        "  return loss_value\n",
        "\n",
        "train_op3 = objax.Jit(train_op_3, gv_3.vars() + tv_3.vars())"
      ],
      "metadata": {
        "id": "3clP9tEGKLd4"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We have 4 hyperparameters and "
      ],
      "metadata": {
        "id": "WV5R0bNj-lkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_MNIST(EPOCHS = 20, BATCH = 32, LEARNING_RATE = 9e-4):\n",
        "  avg_train_loss_epoch = []\n",
        "  avg_val_loss_epoch = []\n",
        "  train_acc_epoch = []\n",
        "  val_acc_epoch = []\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "      avg_train_loss = 0 # (averaged) training loss per batch\n",
        "      avg_val_loss =  0  # (averaged) validation loss per batch\n",
        "      train_acc = 0      # training accuracy per batch\n",
        "      val_acc = 0        # validation accuracy per batch\n",
        "\n",
        "      # shuffle the examples prior to training to remove correlation \n",
        "      train_indices = np.arange(len(X_train)) \n",
        "      np.random.shuffle(train_indices)\n",
        "      for it in range(0, X_train.shape[0], BATCH):\n",
        "          # Put your code here \n",
        "          # Q2\n",
        "          batch = train_indices[it:it+BATCH]\n",
        "          avg_train_loss += float(train_op_3(X_train[batch], Y_train[batch], LEARNING_RATE)[0]) * len(batch)\n",
        "          train_prediction = predict_3(X_train[batch]).argmax(1)\n",
        "          train_acc += (np.array(train_prediction).flatten() == Y_train[batch]).sum()\n",
        "      train_acc_epoch.append(train_acc/X_train.shape[0])\n",
        "      avg_train_loss_epoch.append(avg_train_loss/X_train.shape[0])\n",
        "\n",
        "      # run validation\n",
        "      val_indices = np.arange(len(X_valid)) \n",
        "      np.random.shuffle(val_indices)    \n",
        "      for it in range(0, X_valid.shape[0], BATCH):\n",
        "          # Put your code here \n",
        "          # Q2\n",
        "          batch = val_indices[it:it+BATCH]\n",
        "          avg_val_loss += float(loss_function_m3(X_valid[batch], Y_valid[batch])) * len(batch)\n",
        "          val_prediction = predict_3(X_valid[batch]).argmax(1)\n",
        "          val_acc += (np.array(val_prediction).flatten() == Y_valid[batch]).sum()\n",
        "      val_acc_epoch.append(val_acc/X_valid.shape[0])\n",
        "      avg_val_loss_epoch.append(avg_val_loss/X_valid.shape[0])\n",
        "\n",
        "      print('Epoch %04d  Training Loss %.2f Validation Loss %.2f Training Accuracy %.2f Validation Accuracy %.2f' % (epoch + 1, avg_train_loss/X_train.shape[0], avg_val_loss/X_valid.shape[0], 100*train_acc/X_train.shape[0], 100*val_acc/X_valid.shape[0]))\n",
        "  \n",
        "  #Plot training loss\n",
        "  plt.title(\"Train vs Validation Loss\")\n",
        "  plt.plot(avg_train_loss_epoch, label=\"Train\")\n",
        "  plt.plot(avg_val_loss_epoch, label=\"Validation\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()\n",
        "\n",
        "  plt.title(\"Train vs Validation Accuracy\")\n",
        "  plt.plot(train_acc_epoch, label=\"Train\")\n",
        "  plt.plot(val_acc_epoch, label=\"Validation\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Accuracy (%)\")\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "UoTCK4g5KP22"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_MNIST()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "dgA0TKLB8rAi",
        "outputId": "577dc6ad-15c2-49c7-b9c9-ebf1194708d2"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/objax/nn/layers.py\u001b[0m in \u001b[0;36mrun_layer\u001b[0;34m(layer, f, args, kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/objax/nn/layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mnin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         assert x.shape[1] == nin, (f'Attempting to convolve an input with {x.shape[1]} input channels '\n\u001b[0m\u001b[1;32m    197\u001b[0m                                    \u001b[0;34mf'when the convolution expects {nin} channels. For reference, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Attempting to convolve an input with 28 input channels when the convolution expects 3 channels. For reference, self.w.value.shape=(2, 2, 3, 16) and x.shape=(32, 28, 28).",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mUnfilteredStackTrace\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-a8900f186890>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_MNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-59-c54d5029f0ff>\u001b[0m in \u001b[0;36mtrain_MNIST\u001b[0;34m(EPOCHS, BATCH, LEARNING_RATE)\u001b[0m\n\u001b[1;32m     19\u001b[0m           \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBATCH\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m           \u001b[0mavg_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_op_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m           \u001b[0mtrain_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-2cd0bed8497a>\u001b[0m in \u001b[0;36mtrain_op_3\u001b[0;34m(x, y, learning_rate)\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgv_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# returns gradients, loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0mopt_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/objax/gradient.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m                                            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                                            list(args), kwargs)\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchanges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36mgrad_f_aux\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mgrad_f_aux\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_and_grad_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1098\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36mvalue_and_grad_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1169\u001b[0m       ans, vjp_py, aux = _vjp(\n\u001b[0;32m-> 1170\u001b[0;31m           f_partial, *dyn_args, has_aux=True, reduce_axes=reduce_axes)\n\u001b[0m\u001b[1;32m   1171\u001b[0m     \u001b[0m_check_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36m_vjp\u001b[0;34m(fun, has_aux, reduce_axes, *primals)\u001b[0m\n\u001b[1;32m   2661\u001b[0m     out_primal, out_vjp, aux = ad.vjp(\n\u001b[0;32m-> 2662\u001b[0;31m         flat_fun, primals_flat, has_aux=True, reduce_axes=reduce_axes)\n\u001b[0m\u001b[1;32m   2663\u001b[0m     \u001b[0mout_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_aux_trees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/ad.py\u001b[0m in \u001b[0;36mvjp\u001b[0;34m(traceable, primals, has_aux, reduce_axes)\u001b[0m\n\u001b[1;32m    136\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0mout_primals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinearize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mprimals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/ad.py\u001b[0m in \u001b[0;36mlinearize\u001b[0;34m(traceable, *primals, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0mjvpfun_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjvpfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m   \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_pvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_to_jaxpr_nounits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjvpfun_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_pvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m   \u001b[0mout_primals_pvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tangents_pvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_pvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/profiler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mTraceAnnotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdecorator_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/partial_eval.py\u001b[0m in \u001b[0;36mtrace_to_jaxpr_nounits\u001b[0;34m(fun, pvals, instantiate)\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0mfun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_to_subjaxpr_nounits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstantiate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m     \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout_pvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/objax/gradient.py\u001b[0m in \u001b[0;36mf_func\u001b[0;34m(inputs_and_train_tensors, state_tensors, list_args, kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m                     \u001b[0mlist_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlist_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/objax/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;34m\"\"\"Call the the function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-2cd0bed8497a>\u001b[0m in \u001b[0;36mloss_function_m3\u001b[0;34m(x, labels)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss_function_m3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mobjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_logits_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-ae98324b1257>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/objax/nn/layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/objax/nn/layers.py\u001b[0m in \u001b[0;36mrun_layer\u001b[0;34m(layer, f, args, kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Sequential layer[{layer}] {f} {e}'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnfilteredStackTrace\u001b[0m: AssertionError: Sequential layer[0] objax.nn.Conv2D(nin=3, nout=16, k=(2, 2), strides=(1, 1), dilations=(1, 1), groups=1, padding='SAME', use_bias=True, w_init=kaiming_normal(*, gain=1)) Attempting to convolve an input with 28 input channels when the convolution expects 3 channels. For reference, self.w.value.shape=(2, 2, 3, 16) and x.shape=(32, 28, 28).\n\nThe stack trace below excludes JAX-internal frames.\nThe preceding is the original exception that occurred, unmodified.\n\n--------------------",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-a8900f186890>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_MNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-59-c54d5029f0ff>\u001b[0m in \u001b[0;36mtrain_MNIST\u001b[0;34m(EPOCHS, BATCH, LEARNING_RATE)\u001b[0m\n\u001b[1;32m     18\u001b[0m           \u001b[0;31m# Q2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m           \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBATCH\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m           \u001b[0mavg_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_op_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m           \u001b[0mtrain_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m           \u001b[0mtrain_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_prediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-2cd0bed8497a>\u001b[0m in \u001b[0;36mtrain_op_3\u001b[0;34m(x, y, learning_rate)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_op_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgv_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# returns gradients, loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0mopt_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/objax/gradient.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m         g, (outputs, changes) = self._call(inputs + self.vc.subset(TrainVar).tensors(),\n\u001b[1;32m     83\u001b[0m                                            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                                            list(args), kwargs)\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchanges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/objax/gradient.py\u001b[0m in \u001b[0;36mf_func\u001b[0;34m(inputs_and_train_tensors, state_tensors, list_args, kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_argnums\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                     \u001b[0mlist_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlist_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/objax/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;34m\"\"\"Call the the function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mVarCollection\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-2cd0bed8497a>\u001b[0m in \u001b[0;36mloss_function_m3\u001b[0;34m(x, labels)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss_function_m3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mobjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_logits_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpredict_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmodelMNIST\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-ae98324b1257>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/objax/nn/layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/objax/nn/layers.py\u001b[0m in \u001b[0;36mrun_layer\u001b[0;34m(layer, f, args, kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Sequential layer[{layer}] {f} {e}'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mJaxArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mJaxArray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Sequential layer[0] objax.nn.Conv2D(nin=3, nout=16, k=(2, 2), strides=(1, 1), dilations=(1, 1), groups=1, padding='SAME', use_bias=True, w_init=kaiming_normal(*, gain=1)) Attempting to convolve an input with 28 input channels when the convolution expects 3 channels. For reference, self.w.value.shape=(2, 2, 3, 16) and x.shape=(32, 28, 28)."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eye-uBnQWgc8"
      },
      "source": [
        "##**Problem 4. Open-Ended Exploration**\n",
        "\n",
        "See the handout for instructions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1)\n",
        "# I choose the hyperparameter exploration question, I would go through all the hyperparameter one by one while keeping the other hyperparameters constant. I will then check which value increases my accuracy by maximum and \n",
        "#I choose that hyperparameter. I repeat that for all till I get optimized value for allhyperparameter\n",
        "\n",
        "#2)\n",
        "# We will choose the model with all the hyperparameter's values optimized in part 1) of problem 4\n"
      ],
      "metadata": {
        "id": "e5Z_2gwkybjT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}